\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian inference}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Priors}{2}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Markov chain Monte Carlo (MCMC) in ADMB}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Workflow}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}MCMC Phases}{4}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Output files}{4}{subsection.4.3}}
\newlabel{sec:outfiles}{{4.3}{4}{Output files\relax }{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Meta data: The hst file}{4}{subsubsection.4.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Parameter draws: The psv file}{4}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Derived quantity draws}{4}{subsubsection.4.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Convergence diagnostics}{5}{subsection.4.4}}
\newlabel{sec:diag}{{4.4}{5}{Convergence diagnostics\relax }{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Resuming a chain}{5}{subsection.4.5}}
\newlabel{sec:restart}{{4.5}{5}{Resuming a chain\relax }{subsection.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter draws from the "simple" example. The first columns are from the first half of a chain. The resumed chains contain 10 values from the second half of a chain. The long chain is a long chain that has not been resumed with \texttt  {-mcr}.}}{6}{table.1}}
\newlabel{tab:mcr_table}{{1}{6}{Parameter draws from the "simple" example. The first columns are from the first half of a chain. The resumed chains contain 10 values from the second half of a chain. The long chain is a long chain that has not been resumed with \texttt {-mcr}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Convergence diagnostics}{6}{subsection.4.6}}
\newlabel{sec:diag}{{4.6}{6}{Convergence diagnostics\relax }{subsection.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Starting values}{6}{subsection.4.7}}
\newlabel{sec:startvals}{{4.7}{6}{Starting values\relax }{subsection.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Metropolis-Hastings MCMC}{6}{section.5}}
\newlabel{sec:MH}{{5}{6}{Metropolis-Hastings MCMC\relax }{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Algorithm}{7}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MCMC Arguments}{7}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces ADMB runtime arguments for the Metropolis-Hastings MCMC}}{7}{table.2}}
\newlabel{tab:mh_args}{{2}{7}{ADMB runtime arguments for the Metropolis-Hastings MCMC\relax }{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Console output}{7}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Example MCMC}{8}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The samples from a simple model with \texttt  {mcsave}=1. Note the high autocorrelation of both parameters.The red ellipses show the estimated pairwise parameter covariances, and the red point the MPD.}}{8}{figure.1}}
\newlabel{fig:simple1}{{1}{8}{The samples from a simple model with \texttt {mcsave}=1. Note the high autocorrelation of both parameters.The red ellipses show the estimated pairwise parameter covariances, and the red point the MPD}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Tuning the MH algorithm}{8}{subsection.5.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Thinning rate}{8}{subsubsection.5.5.1}}
\citation{roberts2001}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The samples from a simple model with \texttt  {mcsave}=100. Note the negligible autocorrelation of both parameters.The red ellipses show the estimated pairwise parameter covariances, and the red point the MPD.}}{9}{figure.2}}
\newlabel{fig:simple2}{{2}{9}{The samples from a simple model with \texttt {mcsave}=100. Note the negligible autocorrelation of both parameters.The red ellipses show the estimated pairwise parameter covariances, and the red point the MPD}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Opitmize the acceptance rate}{9}{subsubsection.5.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}mcprobe}{9}{subsubsection.5.5.3}}
\newlabel{sec:mcprobe}{{5.5.3}{9}{mcprobe\relax }{subsubsection.5.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of fat-tailed proposal values for a parameter for the \texttt  {mcprobe} option compared to the default proposal.}}{10}{figure.3}}
\newlabel{fig:mcgrope_example}{{3}{10}{Example of fat-tailed proposal values for a parameter for the \texttt {mcprobe} option compared to the default proposal}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}mcrb}{10}{subsubsection.5.5.4}}
\newlabel{sec:mcrb}{{5.5.4}{10}{mcrb\relax }{subsubsection.5.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The effect of \texttt  {mcrb} on a variety of correlations between two hypothetical parameters. Note that the effect of setting $N=9$ depends on the original correlation.}}{11}{figure.4}}
\newlabel{fig:mcrb}{{4}{11}{The effect of \texttt {mcrb} on a variety of correlations between two hypothetical parameters. Note that the effect of setting $N=9$ depends on the original correlation}{figure.4}{}}
\citation{brooks2011}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}User-supplied covariance matrix}{12}{subsubsection.5.5.5}}
\newlabel{sec:user.cov}{{5.5.5}{12}{User-supplied covariance matrix\relax }{subsubsection.5.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Hyrbid MCMC}{12}{section.6}}
\newlabel{sec:hybrid}{{6}{12}{Hyrbid MCMC\relax }{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Algorithm}{12}{subsection.6.1}}
\newlabel{eq:motion}{{2}{13}{Algorithm\relax }{equation.6.2}{}}
\citation{brooks2011}
\citation{brooks2011}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Leapfrog trajectories for different sets of tuning parameters. The posterior surface is shown as contours, and the posterior mode as a red point. The filled black point is the starting point, and the arrows show the trajectory of the leapfrog steps, ending at the open circle representing the proposed set of parameters.}}{14}{figure.5}}
\newlabel{fig:hybrid_grid_trace}{{5}{14}{Leapfrog trajectories for different sets of tuning parameters. The posterior surface is shown as contours, and the posterior mode as a red point. The filled black point is the starting point, and the arrows show the trajectory of the leapfrog steps, ending at the open circle representing the proposed set of parameters}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Arguments}{14}{subsection.6.2}}
\citation{brooks2011}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Leapfrog projections for different random draws from $U$.}}{15}{figure.6}}
\newlabel{fig:hybrid_seeds}{{6}{15}{Leapfrog projections for different random draws from $U$}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces ADMB runtime arguments for the hybrid MCMC}}{15}{table.3}}
\newlabel{tab:hy_args}{{3}{15}{ADMB runtime arguments for the hybrid MCMC\relax }{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Tuning the hybrid algorithm}{15}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The autocorrelation of the parameter $a$ from the simple model across different tuning parameters of the hybrid method.}}{16}{figure.7}}
\newlabel{fig:hybrid_grid_acf}{{7}{16}{The autocorrelation of the parameter $a$ from the simple model across different tuning parameters of the hybrid method}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}An example with non-linear posterior}{17}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The logistic model with 1 in 100 samples saved using the MH algorithm and estimated covariance matrix (red ellipse).}}{17}{figure.8}}
\newlabel{fig:logistic_mh}{{8}{17}{The logistic model with 1 in 100 samples saved using the MH algorithm and estimated covariance matrix (red ellipse)}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The logistic model with 1 in 100 samples saved using the MH algorithm and empirical covariance matrix (blue ellipse). Note the improvement in acf compared to \ref  {fig:logistic_mh}.}}{18}{figure.9}}
\newlabel{fig:logistic_mh2}{{9}{18}{The logistic model with 1 in 100 samples saved using the MH algorithm and empirical covariance matrix (blue ellipse). Note the improvement in acf compared to \ref {fig:logistic_mh}}{figure.9}{}}
\bibstyle{unsrt}
\bibdata{admb_guide}
\bibcite{roberts2001}{1}
\bibcite{brooks2011}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The logistic model with 1 in 100 samples saved using the MH algorithm and empirical covariance matrix (blue ellipse). Note the improvement in acf compared to \ref  {fig:logistic_mh}.}}{19}{figure.10}}
\newlabel{fig:logistic_mh2}{{10}{19}{The logistic model with 1 in 100 samples saved using the MH algorithm and empirical covariance matrix (blue ellipse). Note the improvement in acf compared to \ref {fig:logistic_mh}}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The logistic model using the hybrid method with default tuning parameters and the estimated covariance matrix (red ellipse).}}{20}{figure.11}}
\newlabel{fig:logistic_hy}{{11}{20}{The logistic model using the hybrid method with default tuning parameters and the estimated covariance matrix (red ellipse)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The logistic model using the hybrid method with default tuning parameters and the empirical covariance matrix (blue ellipse).}}{20}{figure.12}}
\newlabel{fig:logistic_hy2}{{12}{20}{The logistic model using the hybrid method with default tuning parameters and the empirical covariance matrix (blue ellipse)}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The logistic model using the hybrid method and \texttt  {hyeps 0.05} and \texttt  {hynstep 100} for tuning parameters and the empirical covariance matrix (blue ellipse).}}{21}{figure.13}}
\newlabel{fig:logistic_hy3}{{13}{21}{The logistic model using the hybrid method and \texttt {hyeps 0.05} and \texttt {hynstep 100} for tuning parameters and the empirical covariance matrix (blue ellipse)}{figure.13}{}}
